{"cells":[{"cell_type":"markdown","source":["# HW11_part1"],"metadata":{"id":"VMuUMBaJHz2G"}},{"cell_type":"markdown","metadata":{"id":"aFrkHId0Hvh3"},"source":["### Using the TextVectorization layer"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"d77k-4wMHvh3","executionInfo":{"status":"ok","timestamp":1670341727683,"user_tz":-480,"elapsed":25,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["import string\n","\n","class Vectorizer:\n","    def standardize(self, text):\n","        text = text.lower()\n","        return \"\".join(char for char in text if char not in string.punctuation)\n","\n","    def tokenize(self, text):\n","        text = self.standardize(text)\n","        return text.split()\n","\n","    def make_vocabulary(self, dataset):\n","        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n","        for text in dataset:\n","            text = self.standardize(text)\n","            tokens = self.tokenize(text)\n","            for token in tokens:\n","                if token not in self.vocabulary:\n","                    self.vocabulary[token] = len(self.vocabulary)\n","        self.inverse_vocabulary = dict(\n","            (v, k) for k, v in self.vocabulary.items())\n","\n","    def encode(self, text):\n","        text = self.standardize(text)\n","        tokens = self.tokenize(text)\n","        return [self.vocabulary.get(token, 1) for token in tokens]\n","\n","    def decode(self, int_sequence):\n","        return \" \".join(\n","            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n","\n","vectorizer = Vectorizer()\n","dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms.\",\n","]\n","vectorizer.make_vocabulary(dataset)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Htcd-JCHHvh5","executionInfo":{"status":"ok","timestamp":1670341727685,"user_tz":-480,"elapsed":23,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"d94bc7de-f2c3-4617-81a9-8fd8b0e5df98"},"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 3, 5, 7, 1, 5, 6]\n"]}],"source":["test_sentence = \"I write, rewrite, and still rewrite again\"\n","encoded_sentence = vectorizer.encode(test_sentence)\n","print(encoded_sentence)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSz7p24FHvh5","executionInfo":{"status":"ok","timestamp":1670341727686,"user_tz":-480,"elapsed":13,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"e296f8e8-dbb9-4f65-ec46-e0bc37c6c9bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["i write rewrite and [UNK] rewrite again\n"]}],"source":["decoded_sentence = vectorizer.decode(encoded_sentence)\n","print(decoded_sentence)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"iGarytZ8Hvh6","executionInfo":{"status":"ok","timestamp":1670341733609,"user_tz":-480,"elapsed":5932,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["from tensorflow.keras.layers import TextVectorization\n","text_vectorization = TextVectorization(\n","    output_mode=\"int\",\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sYelGAReHvh6","executionInfo":{"status":"ok","timestamp":1670341733609,"user_tz":-480,"elapsed":3,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["import re\n","import string\n","import tensorflow as tf\n","\n","def custom_standardization_fn(string_tensor):\n","    lowercase_string = tf.strings.lower(string_tensor)\n","    return tf.strings.regex_replace(\n","        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n","\n","def custom_split_fn(string_tensor):\n","    return tf.strings.split(string_tensor)\n","\n","text_vectorization = TextVectorization(\n","    output_mode=\"int\",\n","    standardize=custom_standardization_fn,\n","    split=custom_split_fn,\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Ac9KDNHCHvh6","executionInfo":{"status":"ok","timestamp":1670341734286,"user_tz":-480,"elapsed":679,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms.\",\n","]\n","text_vectorization.adapt(dataset)"]},{"cell_type":"markdown","metadata":{"id":"CAuURdiRHvh7"},"source":["**Displaying the vocabulary**"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J_JyEuj0Hvh7","executionInfo":{"status":"ok","timestamp":1670341734286,"user_tz":-480,"elapsed":15,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"f604a34a-8a5b-4ce5-bf20-30bf1aa66b87"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '[UNK]',\n"," 'erase',\n"," 'write',\n"," 'then',\n"," 'rewrite',\n"," 'poppy',\n"," 'i',\n"," 'blooms',\n"," 'and',\n"," 'again',\n"," 'a']"]},"metadata":{},"execution_count":7}],"source":["text_vectorization.get_vocabulary()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"naqhbhg1Hvh7","executionInfo":{"status":"ok","timestamp":1670341734286,"user_tz":-480,"elapsed":12,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"083323cc-084d-4e94-a076-5ca169b4e30b"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"]}],"source":["vocabulary = text_vectorization.get_vocabulary()\n","test_sentence = \"I write, rewrite, and still rewrite again\"\n","encoded_sentence = text_vectorization(test_sentence)\n","print(encoded_sentence)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuFZ7mF7Hvh7","executionInfo":{"status":"ok","timestamp":1670341734287,"user_tz":-480,"elapsed":11,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"f568e9ea-bdd4-4417-d963-df86a2ca6a77"},"outputs":[{"output_type":"stream","name":"stdout","text":["i write rewrite and [UNK] rewrite again\n"]}],"source":["inverse_vocab = dict(enumerate(vocabulary))\n","decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n","print(decoded_sentence)"]},{"cell_type":"markdown","metadata":{"id":"jTe9mwaMHvh8"},"source":["## Two approaches for representing groups of words: Sets and sequences"]},{"cell_type":"markdown","metadata":{"id":"t8m8lQNFHvh8"},"source":["### Preparing the IMDB movie reviews data"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1KFBDStoHvh8","executionInfo":{"status":"ok","timestamp":1670341749797,"user_tz":-480,"elapsed":15520,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"981db5b3-d581-4e68-b9fc-7054c9259ad9"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 80.2M  100 80.2M    0     0  8919k      0  0:00:09  0:00:09 --:--:-- 15.4M\n"]}],"source":["!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"H2PjvWT7Hvh8","executionInfo":{"status":"ok","timestamp":1670341751017,"user_tz":-480,"elapsed":1230,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["!rm -r aclImdb/train/unsup"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RamfvIsHvh8","executionInfo":{"status":"ok","timestamp":1670341751018,"user_tz":-480,"elapsed":6,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"4c894e0d-3c3b-4008-dbb0-62bb8c884483"},"outputs":[{"output_type":"stream","name":"stdout","text":["I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"]}],"source":["!cat aclImdb/train/pos/4077_10.txt"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"U86vxauWHvh9","executionInfo":{"status":"ok","timestamp":1670341751018,"user_tz":-480,"elapsed":3,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["import os, pathlib, shutil, random\n","\n","base_dir = pathlib.Path(\"aclImdb\")\n","val_dir = base_dir / \"val\"\n","train_dir = base_dir / \"train\"\n","for category in (\"neg\", \"pos\"):\n","    os.makedirs(val_dir / category)\n","    files = os.listdir(train_dir / category)\n","    random.Random(1337).shuffle(files)\n","    num_val_samples = int(0.2 * len(files))\n","    val_files = files[-num_val_samples:]\n","    for fname in val_files:\n","        shutil.move(train_dir / category / fname,\n","                    val_dir / category / fname)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXTIjwl9Hvh9","executionInfo":{"status":"ok","timestamp":1670341753953,"user_tz":-480,"elapsed":2937,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"41088948-f636-471f-b1dd-a147d9c6aeaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 20000 files belonging to 2 classes.\n","Found 5000 files belonging to 2 classes.\n","Found 25000 files belonging to 2 classes.\n"]}],"source":["from tensorflow import keras\n","batch_size = 32\n","\n","train_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/train\", batch_size=batch_size\n",")\n","val_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/val\", batch_size=batch_size\n",")\n","test_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/test\", batch_size=batch_size\n",")"]},{"cell_type":"markdown","metadata":{"id":"YH3prbLQHvh9"},"source":["**Displaying the shapes and dtypes of the first batch**"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vjPFfDaHvh9","executionInfo":{"status":"ok","timestamp":1670341753953,"user_tz":-480,"elapsed":5,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"71aea5b5-782d-4f3a-e50f-e0ac937a5fb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape: (32,)\n","inputs.dtype: <dtype: 'string'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor(b\"My main comment on this movie is how Zwick was able to get credible actors to work on this movie? Impressive cast \\xc2\\x96 even for the supporting characters, none of which helps this movie really. I have to admit though, Tom Hank's cameo almost made it worth it \\xc2\\x96 what was that about Tom? Did you lose a bet? The best cameo of the movie was Joe Isuzu though - by far a classic! The premise is good. Basinger's character, struggling with existence as a Pink Lady, is making her way toward Vegas motel by motel pitching the glorious pyramid of cosmetic sales. This happens as Corbett's character is on his way to Vegas to deliver an Elvis suit to his soon to be ex-wife motivated by\\xc2\\x85.what else\\xc2\\x85.extortion. As they both make their way, they have numerous run-ins with Elvis impersonators who on their way to an Elvis impersonating convention in Vegas. Soon, the FBI gets involved and begins to track what they think is an Elvis impersonator serial killer. Unfortunately, premise doesn't mean the movie was good.<br /><br />When watching this movie, imagine you are back in the first grade \\xc2\\x96 when story lines and continuity aren't really important. It is much more enjoyable to just watch Basinger look beautiful in her Pink Lady outfit rather than wondering why what she is doing doesn't really make sense. The movie tries hard, but ultimately falls way way way short. Ultimately, it is filled with ideas that could have theoretically been funny but in practice were not that funny.<br /><br />It isn't the worst, but you may find you yourself feel like leaving the building when watching this one\\xc2\\x85\\xc2\\x85 Don't say I didn't warn you!\", shape=(), dtype=string)\n","targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"]}],"source":["for inputs, targets in train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"TbfvXTEqHvh9"},"source":["### Processing words as a set: The bag-of-words approach"]},{"cell_type":"markdown","metadata":{"id":"q1ERyJQRHvh9"},"source":["#### Single words (unigrams) with binary encoding"]},{"cell_type":"markdown","metadata":{"id":"fahHWsWVHvh-"},"source":["**Preprocessing our datasets with a `TextVectorization` layer**"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"-Z5Cb9jqHvh-","executionInfo":{"status":"ok","timestamp":1670341759715,"user_tz":-480,"elapsed":5765,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["text_vectorization = TextVectorization(\n","    max_tokens=20000,\n","    output_mode=\"multi_hot\",\n",")\n","text_only_train_ds = train_ds.map(lambda x, y: x)\n","text_vectorization.adapt(text_only_train_ds)\n","\n","binary_1gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_1gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_1gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)"]},{"cell_type":"markdown","metadata":{"id":"2tKUTcHLHvh-"},"source":["**Inspecting the output of our binary unigram dataset**"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFitcXTwHvh-","executionInfo":{"status":"ok","timestamp":1670341759719,"user_tz":-480,"elapsed":17,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"2a7c9753-d2ad-47df-bde1-a22dff72e721"},"outputs":[{"output_type":"stream","name":"stdout","text":["inputs.shape: (32, 20000)\n","inputs.dtype: <dtype: 'float32'>\n","targets.shape: (32,)\n","targets.dtype: <dtype: 'int32'>\n","inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n","targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"]}],"source":["for inputs, targets in binary_1gram_train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"i34Zz3ZyHvh-"},"source":["**Our model-building utility**"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"t0gWm1gGHvh-","executionInfo":{"status":"ok","timestamp":1670341759719,"user_tz":-480,"elapsed":15,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","def get_model(max_tokens=20000, hidden_dim=16):\n","    inputs = keras.Input(shape=(max_tokens,))\n","    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n","    x = layers.Dropout(0.5)(x)\n","    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","    model = keras.Model(inputs, outputs)\n","    model.compile(optimizer=\"rmsprop\",\n","                  loss=\"binary_crossentropy\",\n","                  metrics=[\"accuracy\"])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"97ef8FBcHvh-"},"source":["**Training and testing the binary unigram model**"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PI8esGrhHvh_","executionInfo":{"status":"ok","timestamp":1670341814091,"user_tz":-480,"elapsed":54386,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"3dafe90a-470b-4fcb-bb85-5fa27dd350ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 20000)]           0         \n","                                                                 \n"," dense (Dense)               (None, 16)                320016    \n","                                                                 \n"," dropout (Dropout)           (None, 16)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 320,033\n","Trainable params: 320,033\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 10s 12ms/step - loss: 0.3995 - accuracy: 0.8292 - val_loss: 0.2835 - val_accuracy: 0.8900\n","Epoch 2/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2656 - accuracy: 0.8997 - val_loss: 0.2786 - val_accuracy: 0.8946\n","Epoch 3/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2441 - accuracy: 0.9171 - val_loss: 0.2922 - val_accuracy: 0.8950\n","Epoch 4/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2237 - accuracy: 0.9237 - val_loss: 0.3066 - val_accuracy: 0.8956\n","Epoch 5/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2221 - accuracy: 0.9296 - val_loss: 0.3202 - val_accuracy: 0.8962\n","Epoch 6/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2159 - accuracy: 0.9298 - val_loss: 0.3323 - val_accuracy: 0.8906\n","Epoch 7/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2123 - accuracy: 0.9354 - val_loss: 0.3460 - val_accuracy: 0.8930\n","Epoch 8/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2053 - accuracy: 0.9334 - val_loss: 0.3525 - val_accuracy: 0.8924\n","Epoch 9/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2068 - accuracy: 0.9380 - val_loss: 0.3617 - val_accuracy: 0.8940\n","Epoch 10/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2096 - accuracy: 0.9373 - val_loss: 0.3676 - val_accuracy: 0.8936\n","782/782 [==============================] - 7s 9ms/step - loss: 0.2866 - accuracy: 0.8916\n","Test acc: 0.892\n"]}],"source":["model = get_model()\n","model.summary()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(binary_1gram_train_ds.cache(),\n","          validation_data=binary_1gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model(\"binary_1gram.keras\")\n","print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"0EOo7N6QHvh_"},"source":["#### Bigrams with binary encoding"]},{"cell_type":"markdown","metadata":{"id":"MuU2VmCCHvh_"},"source":["**Configuring the `TextVectorization` layer to return bigrams**"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"APqOniPaHvh_","executionInfo":{"status":"ok","timestamp":1670341814092,"user_tz":-480,"elapsed":19,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode=\"multi_hot\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"Yo775-43Hvh_"},"source":["**Training and testing the binary bigram model**"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ximKt1nHvh_","executionInfo":{"status":"ok","timestamp":1670341891712,"user_tz":-480,"elapsed":77638,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"6fbb8a2f-3553-420d-a42a-65e2ab918079"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 20000)]           0         \n","                                                                 \n"," dense_2 (Dense)             (None, 16)                320016    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 16)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 320,033\n","Trainable params: 320,033\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 10s 15ms/step - loss: 0.3998 - accuracy: 0.8334 - val_loss: 0.2781 - val_accuracy: 0.8952\n","Epoch 2/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2567 - accuracy: 0.9094 - val_loss: 0.2802 - val_accuracy: 0.9018\n","Epoch 3/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2250 - accuracy: 0.9245 - val_loss: 0.3103 - val_accuracy: 0.8972\n","Epoch 4/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2049 - accuracy: 0.9315 - val_loss: 0.3305 - val_accuracy: 0.8958\n","Epoch 5/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2044 - accuracy: 0.9362 - val_loss: 0.3464 - val_accuracy: 0.8950\n","Epoch 6/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.1997 - accuracy: 0.9395 - val_loss: 0.3563 - val_accuracy: 0.8936\n","Epoch 7/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.1854 - accuracy: 0.9411 - val_loss: 0.3824 - val_accuracy: 0.8922\n","Epoch 8/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.1951 - accuracy: 0.9415 - val_loss: 0.3928 - val_accuracy: 0.8910\n","Epoch 9/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.1821 - accuracy: 0.9467 - val_loss: 0.3943 - val_accuracy: 0.8904\n","Epoch 10/10\n","625/625 [==============================] - 3s 5ms/step - loss: 0.1877 - accuracy: 0.9467 - val_loss: 0.4203 - val_accuracy: 0.8880\n","782/782 [==============================] - 8s 10ms/step - loss: 0.2781 - accuracy: 0.8957\n","Test acc: 0.896\n"]}],"source":["text_vectorization.adapt(text_only_train_ds)\n","binary_2gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_2gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","binary_2gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","\n","model = get_model()\n","model.summary()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(binary_2gram_train_ds.cache(),\n","          validation_data=binary_2gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model(\"binary_2gram.keras\")\n","print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"6LeSh0mMHviA"},"source":["#### Bigrams with TF-IDF encoding"]},{"cell_type":"markdown","metadata":{"id":"eKEd2qPkHviA"},"source":["**Configuring the `TextVectorization` layer to return token counts**"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"M0pLoEQhHviA","executionInfo":{"status":"ok","timestamp":1670341891713,"user_tz":-480,"elapsed":18,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode=\"count\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"DfckpWLmHviA"},"source":["**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"GSy6m7P3HviA","executionInfo":{"status":"ok","timestamp":1670341891713,"user_tz":-480,"elapsed":17,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["text_vectorization = TextVectorization(\n","    ngrams=2,\n","    max_tokens=20000,\n","    output_mode=\"tf_idf\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"UM46OZrfHviA"},"source":["**Training and testing the TF-IDF bigram model**"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ctwDCSqHviA","executionInfo":{"status":"ok","timestamp":1670341965716,"user_tz":-480,"elapsed":74020,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"77f4c7ee-bc11-49a7-bbe3-bb3102a1c395"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 20000)]           0         \n","                                                                 \n"," dense_4 (Dense)             (None, 16)                320016    \n","                                                                 \n"," dropout_2 (Dropout)         (None, 16)                0         \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 320,033\n","Trainable params: 320,033\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 9s 13ms/step - loss: 0.4986 - accuracy: 0.7597 - val_loss: 0.3547 - val_accuracy: 0.8716\n","Epoch 2/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.3528 - accuracy: 0.8307 - val_loss: 0.3187 - val_accuracy: 0.8916\n","Epoch 3/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.3176 - accuracy: 0.8533 - val_loss: 0.3035 - val_accuracy: 0.8964\n","Epoch 4/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2958 - accuracy: 0.8632 - val_loss: 0.3273 - val_accuracy: 0.8802\n","Epoch 5/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2761 - accuracy: 0.8709 - val_loss: 0.3879 - val_accuracy: 0.8530\n","Epoch 6/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2647 - accuracy: 0.8737 - val_loss: 0.3586 - val_accuracy: 0.8712\n","Epoch 7/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2660 - accuracy: 0.8862 - val_loss: 0.3635 - val_accuracy: 0.8786\n","Epoch 8/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2366 - accuracy: 0.8960 - val_loss: 0.3754 - val_accuracy: 0.8870\n","Epoch 9/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2311 - accuracy: 0.9058 - val_loss: 0.3722 - val_accuracy: 0.8888\n","Epoch 10/10\n","625/625 [==============================] - 3s 4ms/step - loss: 0.2183 - accuracy: 0.9092 - val_loss: 0.3900 - val_accuracy: 0.8958\n","782/782 [==============================] - 8s 10ms/step - loss: 0.2912 - accuracy: 0.8933\n","Test acc: 0.893\n"]}],"source":["text_vectorization.adapt(text_only_train_ds)\n","\n","tfidf_2gram_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","tfidf_2gram_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","tfidf_2gram_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","\n","model = get_model()\n","model.summary()\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(tfidf_2gram_train_ds.cache(),\n","          validation_data=tfidf_2gram_val_ds.cache(),\n","          epochs=10,\n","          callbacks=callbacks)\n","model = keras.models.load_model(\"tfidf_2gram.keras\")\n","print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"dxByL_chHviB","executionInfo":{"status":"ok","timestamp":1670341965717,"user_tz":-480,"elapsed":25,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}}},"outputs":[],"source":["inputs = keras.Input(shape=(1,), dtype=\"string\")\n","processed_inputs = text_vectorization(inputs)\n","outputs = model(processed_inputs)\n","inference_model = keras.Model(inputs, outputs)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZ2j29ugHviB","executionInfo":{"status":"ok","timestamp":1670341965718,"user_tz":-480,"elapsed":25,"user":{"displayName":"妙妙屋","userId":"08170026472488033073"}},"outputId":"a686b1d2-8883-407a-8128-3f47dafd5faf"},"outputs":[{"output_type":"stream","name":"stdout","text":["94.30 percent positive\n"]}],"source":["import tensorflow as tf\n","raw_text_data = tf.convert_to_tensor([\n","    [\"That was an excellent movie, I loved it.\"],\n","])\n","predictions = inference_model(raw_text_data)\n","print(f\"{float(predictions[0] * 100):.2f} percent positive\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}